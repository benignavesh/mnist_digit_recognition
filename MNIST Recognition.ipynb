{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from skimage import transform,io\n",
    "#import scipy.ndimage as ndimage\n",
    "#import scipy\n",
    "#import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(features_train, labels_train), (features_test, labels_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.astype('float32')\n",
    "features_test = features_test.astype('float32')\n",
    "features_train /= 255\n",
    "features_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = keras.utils.to_categorical(labels_train, 10)\n",
    "labels_test = keras.utils.to_categorical(labels_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fdac55cd68>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features_train[80].reshape([28,28]), cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "out = labels_train[80]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels Last.\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "from keras import backend as K \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    #features_train = features_train.reshape(features_train.shape[0],1, img_rows, img_cols)\n",
    "    #features_test = features_test.reshape(features_test.shape[0],1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "    \n",
    "else:\n",
    "    features_train = features_train.reshape(features_train.shape[0], img_rows, img_cols, 1)\n",
    "    features_test = features_test.reshape(features_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    print(\"Channels Last.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu',input_shape = input_shape))\n",
    "model.add(Conv2D(32,(5,5), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model.fit(features_train, labels_train, epochs = 1, validation_data = (features_test, labels_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(features_test, labels_test, verbose = 0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test Accuracy: \", score[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "468/468 [==============================] - 18s 39ms/step - loss: 0.5457 - acc: 0.8240 - val_loss: 0.0524 - val_acc: 0.9829\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(features_train,labels_train, batch_size=128),\n",
    "                              epochs = 1, validation_data = (features_test,labels_test),\n",
    "                              verbose = 1, steps_per_epoch=features_train.shape[0] // 128\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'recognition/5.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2901e599407c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recognition/5.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# returns image object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m#out_img = imread('output.png')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageOps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#out_img = np.invert(out_img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2580\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'recognition/5.png'"
     ]
    }
   ],
   "source": [
    "im = Image.open('output.png').convert(\"L\") # returns image object\n",
    "    #out_img = imread('output.png')\n",
    "im = PIL.ImageOps.invert(im) \n",
    "    #out_img = np.invert(out_img)\n",
    "im = im.resize((28,28))\n",
    "    #out_img = imresize(out_img, (28,28))\n",
    "im = np.reshape(im, (1,28,28,1))\n",
    "#plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "out =model.predict(im)\n",
    "print(out)\n",
    "print(np.argmax(out,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json() \n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print('Model saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
